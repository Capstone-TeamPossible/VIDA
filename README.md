# VIDA: Vision-Informed Deep Audio Feature Generation for Action Recognition

ë³¸ ì—°êµ¬ëŠ” ì˜¤ë””ì˜¤ ëª¨ë‹¬ë¦¬í‹°ê°€ ê²°ì†ëœ ë¹„ë””ì˜¤ ê¸°ë°˜ í–‰ë™ ì¸ì‹ í™˜ê²½ì—ì„œ, ì‹œê° ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ì •í•©í•œ ì˜¤ë””ì˜¤ í”¼ì²˜ë¥¼ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ì¸ VIDA (Vision-Informed Deep Audio Feature Generation for Action Recognition)ë¥¼ ì œì•ˆí•œë‹¤. ì œì•ˆí•˜ëŠ” í”„ë ˆì„ì›Œí¬ëŠ” SVDMapê³¼ CAFMapì˜ ë‘ ì•„í‚¤í…ì²˜ë¡œ êµ¬ì„±ë˜ë©°, ëª¨ë‘ Transformer ê¸°ë°˜ ì‹œê° í”¼ì²˜ ì¶”ì¶œê¸°ì™€ LSTM ê¸°ë°˜ ì˜¤ë””ì˜¤ ìƒì„±ê¸°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì˜ë¯¸ ì •í•©ì„±ì„ í•™ìŠµ ê³¼ì •ì— í†µí•©í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ í‘œí˜„ì˜ ì‹ ë¢°ì„±ê³¼ í™œìš©ë„ë¥¼ ë†’ì¸ë‹¤. Moments-in-Time ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹¤í—˜ ê²°ê³¼, ì‹¤ì œ ì˜¤ë””ì˜¤ê°€ ì—†ëŠ” ìƒí™©ì—ì„œë„ ìƒì„±ëœ ì˜¤ë””ì˜¤ í”¼ì²˜ë¥¼ í™œìš©í–ˆì„ ë•Œ SVDMapì€ ì•½ 35%, CAFMapì€ ì•½ 31%ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ì˜€ë‹¤. ì´ëŠ” ê¸°ì¡´ ì˜¤ë””ì˜¤ ë¯¸í¬í•¨ ëª¨ë¸ ëŒ€ë¹„ ì•ˆì •ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¸ ê²ƒìœ¼ë¡œ, MiT ë°ì´í„°ì…‹ì˜ SOTA ì •í™•ë„ì¸ 53%ì—ëŠ” ë„ë‹¬í•˜ì§€ ëª»í–ˆìœ¼ë‚˜, ì˜¤ë””ì˜¤ ê²°ì† í™˜ê²½ì—ì„œë„ ìœ ì˜ë¯¸í•œ í–‰ë™ ì¸ì‹ ì„±ëŠ¥ì„ í™•ë³´í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.

---

## Repository êµ¬ì¡°

```
â”œâ”€â”€ SVDMap/                     # ì•„í‚¤í…ì²˜ 1 ì½”ë“œ
â”‚   â””â”€â”€ model.ipynb             # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ ~ ë¶„ë¥˜)
â”‚   â””â”€â”€ arc1_env.yml            # ì•„í‚¤í…ì²˜ 1 Conda í™˜ê²½ íŒŒì¼
â”‚   â””â”€â”€ data/                   # ì‹¤í—˜ì— í•„ìš”í•œ ì…ë ¥ ë° ì¶œë ¥ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
â”‚       â”œâ”€â”€ SAVLD.csv
â”‚       â””â”€â”€ pred_test.json
â”œâ”€â”€ CAFMap/                     # ì•„í‚¤í…ì²˜ 2 ì½”ë“œ
â”‚   â””â”€â”€ model.ipynb             # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ ~ ë¶„ë¥˜)
â”‚   â””â”€â”€ arc2_env.yaml           # ì•„í‚¤í…ì²˜ 2 Conda í™˜ê²½ íŒŒì¼
â”œâ”€â”€ data-example/               # ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œ
â”œâ”€â”€ data-preprocessing/         # ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œ
â”œâ”€â”€ feature-extraction/         # ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ í”¼ì²˜ ì „ì²˜ë¦¬ ì½”ë“œ
â””â”€â”€ README.md                   # ì„¤ëª… íŒŒì¼
```

---

## í™˜ê²½ ì„¤ì • (How to build)

```bash
# arc1 Conda í™˜ê²½ ìƒì„±
conda env create -f arc1_env.yml
conda activate arc1_env

# arc2 Conda í™˜ê²½ ìƒì„±
conda env create -f arc2_env.yaml
conda activate arc1_env
```

> ëª¨ë“  ì‹¤í—˜ì€ ìœ„ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤. `transformers`, `torch`, `torchaudio`, `opencv`, `scikit-learn`, `librosa` ë“±ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì•ˆë‚´

ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê° ëª¨ë“ˆì˜ ì‚°ì¶œë¬¼ `.npy` ë° `.json` ë°ì´í„° íŒŒì¼ì€ ì´ ìš©ëŸ‰ ì•½ 1.6GB ì´ìƒìœ¼ë¡œ GitHubì— ì§ì ‘ ì—…ë¡œë“œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.  
ì‹¤í–‰ì„ ìœ„í•´ í•„ìš”í•œ íŒŒì¼ ëª©ë¡ ë° ìœ„ì¹˜ëŠ” ì•„ë˜ ê° ë‹¨ê³„ì— ìƒì„¸íˆ ê¸°ìˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

ë¹„ë””ì˜¤ í–‰ë™ ì¸ì‹ì„ ìœ„í•œ ì›ë³¸ ë°ì´í„°ì˜ í¬ê¸°ëŠ” 275GBë¡œ ì‹¤í—˜ í™˜ê²½ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.
ì›ë³¸ ë°ì´í„°ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë©° ì‹¤í—˜ ì¬í˜„ì„ ìœ„í•´ì„œëŠ” `preprocessing/` ë””ë ‰í† ë¦¬ ë‚´ ì½”ë“œë¥¼ í™œìš©í•œ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

[Moments in Time Dataset](http://moments.csail.mit.edu/)

---

## â–¶ï¸ Architecture 1: SVDMap

ë³¸ ì•„í‚¤í…ì²˜ëŠ” LSTMìœ¼ë¡œ ì˜¤ë””ì˜¤ í”¼ì²˜ë¥¼ ìƒì„±í•œ ë’¤, AST ëª¨ë¸ì„ í†µí•´ multi-label predictionì„ ìˆ˜í–‰í•˜ê³ , Semantic Dictionaryì™€ì˜ ì˜ë¯¸ ì •í•©ì„± ê¸°ë°˜ í•„í„°ë§ì„ í†µí•´ ìœ íš¨í•œ í”¼ì²˜ë§Œì„ í–‰ë™ ì¸ì‹ì— í™œìš©í•©ë‹ˆë‹¤.

---

### 1. LSTM ê¸°ë°˜ ì˜¤ë””ì˜¤ í”¼ì²˜ ìƒì„±

- í•„ìš”í•œ íŒŒì¼:  
  `audio_filtered_train.npy`, `audio_filtered_val.npy`, `audio_filtered_test.npy`  
  `rgb_filtered_train.npy`, `rgb_filtered_val.npy`, `rgb_filtered_test.npy`  
  `flow_filtered_train.npy`, `flow_filtered_val.npy`, `flow_filtered_test.npy`  
  â†’ ì´ 9ê°œ

- ì‹¤í–‰ íŒŒì¼:  
  `SVDMap/model.ipynb`

- ì¶œë ¥:  
  `gen_audio_train.npy`, `gen_audio_val.npy`, `gen_audio_test.npy`

---

### 2. ì˜¤ë””ì˜¤ ê¸°ë°˜ í–‰ë™ ë¶„ë¥˜

- í•„ìš”í•œ íŒŒì¼:  
  ìœ„ì—ì„œ ìƒì„±ëœ `gen_audio_*.npy` 3ê°œ

- ì‹¤í–‰ íŒŒì¼:  
  `SVDMap/model.ipynb`

- ì¶œë ¥:  
  Test accuracy, Confusion Matrix

---

### 3. AST ê¸°ë°˜ ë©€í‹°ë¼ë²¨ ì˜ˆì¸¡

- í•„ìš”í•œ íŒŒì¼:  
  `audio_waveform_test.npy`

- ì‹¤í–‰ íŒŒì¼:  
  `SVDMap/model.ipynb`

- ì¶œë ¥:  
  `pred_test.json`, `ast_pred_test_top5.json`

---

### 4. IoU ê¸°ë°˜ ì˜ë¯¸ ì •í•©ì„± í•„í„°ë§

- í•„ìš”í•œ íŒŒì¼:  
  `SAVLD.csv`, `pred_test.json`

- ì‹¤í–‰ íŒŒì¼:  
  `SVDMap/model.ipynb`

- ì¶œë ¥:  
  `audio_filtered_train_selected.npy`, `...val_selected.npy`, `...test_selected.npy` ë“± ì´ 9ê°œ

---

### 5~6. Filtering ê¸°ë°˜ Action Classifier í•™ìŠµ

- í•„ìš”í•œ íŒŒì¼:  
  Filteringì„ ê±°ì¹œ selected `.npy` ì´ 9ê°œ

- ì‹¤í–‰ íŒŒì¼:  
  `SVDMap/model.ipynb`

- ì¶œë ¥:  
  Final classification accuracy using filtered audio

---

## â–¶ï¸ Architecture 2: CAFMap

ë³¸ ì•„í‚¤í…ì²˜ëŠ” BLIP ê¸°ë°˜ ìº¡ì…˜ê³¼ AudioSet í´ë˜ìŠ¤ ê°„ ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤í•‘ì„ í†µí•´ ê° í”„ë ˆì„ì˜ ì˜¤ë””ì˜¤ ì¡°ê±´ì„ êµ¬ì„±í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LSTMì„ í†µí•´ ì‹œê³„ì—´ ì˜¤ë””ì˜¤ í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´í›„ í•´ë‹¹ í”¼ì²˜ë¥¼ í™œìš©í•˜ì—¬ ì•¡ì…˜ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

---

### 1. ëŒ€í‘œ í”„ë ˆì„ ê¸°ë°˜ ìº¡ì…˜ ìƒì„±

- í•„ìš”í•œ íŒŒì¼:  
  ì „ì²˜ë¦¬í•œ ê° í´ë˜ìŠ¤ ë³„ RGB í”„ë ˆì„ ì´ë¯¸ì§€

- ì‹¤í–‰ íŒŒì¼:  
  `CAFMap/model.ipynb`

- ì¶œë ¥:  
  `{split}_{class_name}_captions.json`
  â†’ ê° ë¹„ë””ì˜¤ì˜ ëŒ€í‘œ í”„ë ˆì„ ë° ëŒ€ì‘ ìº¡ì…˜ ì •ë³´ ì €ì¥

---

### 2. AudioSet í‰ê·  í”¼ì²˜ ì²˜ë¦¬

- í•„ìš”í•œ íŒŒì¼:  
  [AudioSet TFRecord](https://research.google.com/audioset/download.html) ì¤‘ `bal_train/*.tfrecord`
  `class_labels_indices.csv`, `balanced_train_segments.csv`

- ì‹¤í–‰ íŒŒì¼:  
  `CAFMap/model.ipynb`

- ì¶œë ¥:  
  `audioset_class_embeddings.npy`
  â†’ ê° AudioSet í´ë˜ìŠ¤ì˜ í‰ê·  ì˜¤ë””ì˜¤ í”¼ì²˜ ì €ì¥

---

### 3. ìº¡ì…˜ â†’ ì˜¤ë””ì˜¤ í”¼ì²˜ ë§¤í•‘ ë° condition dictionary ìƒì„±

- í•„ìš”í•œ íŒŒì¼:  
  ìƒì„±ëœ ìº¡ì…˜ JSON íŒŒì¼, `audioset_class_embeddings.npy`

- ì‹¤í–‰ íŒŒì¼:  
  `CAFMap/model.ipynb`

- ì¶œë ¥:  
  `condition_dict.npy`
  â†’ {(class_name, video_id): {"mask": [18], "features": [18, 128]}}

---

### 4. LSTM ê¸°ë°˜ ì˜¤ë””ì˜¤ í”¼ì²˜ ìƒì„±ê¸° í•™ìŠµ

- í•„ìš”í•œ íŒŒì¼:  
  `condition_dict.npy`
  GT ì˜¤ë””ì˜¤ `audio_filtered_train.npy`, `audio_filtered_val.npy`

- ì‹¤í–‰ íŒŒì¼:  
  `CAFMap/model.ipynb`

- ì¶œë ¥:  
  `gen_audio_v1_train.npy', `gengen_audio_v1_val.npy'
  â†’ ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ [18, 128] ì‹œí€€ìŠ¤ ìƒì„± ê²°ê³¼ ì €ì¥

---

### 5. ì˜¤ë””ì˜¤ ê¸°ë°˜ Action Classification

- í•„ìš”í•œ íŒŒì¼:  
  `18-audio-train.csv`, `20_class.txt`, `rgb_filtered_*.npy`, `flow_filtered_*.npy`, `audio_filtered_*.npy`, `gen_audio_v1_*.npy`

- ì‹¤í–‰ íŒŒì¼:  
  `CAFMap/model.ipynb`

- ì¶œë ¥:  
  Ground Truth ê¸°ë°˜ Classification ì •í™•ë„, Generated Audio ê¸°ë°˜ Classification ì •í™•ë„, Vision-only ê¸°ì¤€ ì •í™•ë„

---

## ì¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­

- ëª¨ë“  ì‹¤í—˜ì€ `arc1_env.yml` ë˜ëŠ” `arc2_env.yaml` ê¸°ë°˜ Conda í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- ê° ë„¤íŠ¸ì›Œí¬ì˜ ì¤‘ê°„ ì‚°ì¶œë¬¼ `.npy` íŒŒì¼ì€ ìš©ëŸ‰ ì œí•œìœ¼ë¡œ GitHubì— í¬í•¨ë˜ì§€ ì•Šì•˜ìœ¼ë©°,  
  ì „ì²˜ë¦¬ ì½”ë“œ ë˜ëŠ” ì™¸ë¶€ ë§í¬ë¥¼ í†µí•´ ìƒì„± ê°€ëŠ¥
- ì‹¤í–‰ íë¦„ì€ ëª¨ë‘ `SVDMap/model.ipynb` ë˜ëŠ” `CAFMap/model.ipynb` ë‚´ë¶€ ì…€ì—ì„œ ë‹¨ê³„ë³„ë¡œ ì§„í–‰ ê°€ëŠ¥

---

## ê²°ê³¼

ì‹¤í—˜ ìµœì¢… ê²°ê³¼ë¬¼ì€ ë…¼ë¬¸ìœ¼ë¡œ, ì²¨ë¶€ëœ íŒŒì¼ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
[ë…¼ë¬¸ ë³´ê¸°](VIDA_Vision-Informed-Deep-Audio-Feature-Generation-for-Action-Recognition.pdf)

- **SVDMap**
  
| Model                   | Accuracy |
|------------------------|----------|
| Baseline          | 17.7%    |
| Filtered Audio   | **35.6%**|
| Ground Truth Audio         | 34.3%    |

- **CAFMap**
  
| Model                   | Accuracy |
|------------------------|----------|
| Baseline           | 22.5%    |
| Generated Audio   | **30.7%%**|
| Ground Truth Audio         | 33.9%    |

---

## ğŸ‘©ğŸ»â€ğŸ’» íŒ€ì› ì†Œê°œ

- **ê¹€ì§€ì€** (2140010) â€“ CAFMap ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° êµ¬í˜„  
- **ìœ¤ì„œì•„** (2168019) â€“ SVDMap ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° êµ¬í˜„  
- **ì¥ì€ì„±** (2271052) â€“ ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ ë° feature extractor ì„¤ê³„  

ì´í™”ì—¬ìëŒ€í•™êµ, 2025ë…„ ìº¡ìŠ¤í†¤ë””ìì¸

---

## ë¼ì´ì„ ìŠ¤

ë³¸ í”„ë¡œì íŠ¸ì˜ ì½”ë“œëŠ” êµìœ¡ ë° ë¹„ì˜ë¦¬ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µê°œë©ë‹ˆë‹¤.  
