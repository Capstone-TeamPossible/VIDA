{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM audio feature generation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed .npy Files 불러오기 \n",
    "\n",
    "base_path = os.path.join(os.path.dirname(__file__), 'data') if '__file__' in globals() else 'data/'\n",
    "audio_train = np.load(os.path.join(base_path, \"audio_filtered_train.npy\"), allow_pickle=True).item()\n",
    "audio_val = np.load(os.path.join(base_path, \"audio_filtered_val.npy\"), allow_pickle=True).item()\n",
    "audio_test = np.load(os.path.join(base_path, \"audio_filtered_test.npy\"), allow_pickle=True).item()\n",
    "\n",
    "rgb_train = np.load(os.path.join(base_path, \"rgb_filtered_train.npy\"), allow_pickle=True).item()\n",
    "rgb_val = np.load(os.path.join(base_path, \"rgb_filtered_val.npy\"), allow_pickle=True).item()\n",
    "rgb_test = np.load(os.path.join(base_path, \"rgb_filtered_test.npy\"), allow_pickle=True).item()\n",
    "\n",
    "flow_train = np.load(os.path.join(base_path, \"flow_filtered_train.npy\"), allow_pickle=True).item()\n",
    "flow_val = np.load(os.path.join(base_path, \"flow_filtered_val.npy\"), allow_pickle=True).item()\n",
    "flow_test = np.load(os.path.join(base_path, \"flow_filtered_test.npy\"), allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의\n",
    "\n",
    "class VideoToAudioDataset(Dataset):\n",
    "    def __init__(self, rgb_dict, flow_dict, audio_dict):\n",
    "        self.keys = list(audio_dict.keys())\n",
    "        self.rgb_dict = rgb_dict\n",
    "        self.flow_dict = flow_dict\n",
    "        self.audio_dict = audio_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        rgb_feat = self.rgb_dict[key]      # (18, 768)\n",
    "        flow_feat = self.flow_dict[key]    # (18, 768)\n",
    "        x = np.concatenate([rgb_feat, flow_feat], axis=-1)  # (18, 1536)\n",
    "        y = self.audio_dict[key]           # (18, 128)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 \n",
    "\n",
    "train_loader = DataLoader(VideoToAudioDataset(rgb_train, flow_train, audio_train), batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(VideoToAudioDataset(rgb_val, flow_val, audio_val), batch_size=4)\n",
    "test_loader = DataLoader(VideoToAudioDataset(rgb_test, flow_test, audio_test), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-based generator \n",
    "\n",
    "class AudioFeatureGenerator(nn.Module):\n",
    "    def __init__(self, input_dim=1536, hidden_dim=512, num_layers=2, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # (B, 18, H)\n",
    "        return self.fc(out)    # (B, 18, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Validation Functions\n",
    "\n",
    "def sequence_mse_loss(pred, target):\n",
    "    return ((pred - target) ** 2).mean()\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y, _ in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)  # (B, 18, 218)\n",
    "        loss = criterion(pred, y)  # element-wise comparison\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, criterion, device, visualize=False, num_samples=3):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(loader.dataset)):\n",
    "            x, y, key = loader.dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if visualize and len(samples) < num_samples:\n",
    "                samples.append((key, pred.squeeze(0).cpu().numpy(), y.squeeze(0).cpu().numpy()))\n",
    "\n",
    "    if visualize:\n",
    "        visualize_audio_similarity_samples(samples)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def visualize_audio_similarity_samples(samples):\n",
    "    for key, pred, gt in samples:\n",
    "        cosine_sim = [cosine_similarity(pred[i:i+1], gt[i:i+1])[0, 0] for i in range(18)]\n",
    "        mse = [np.mean((pred[i] - gt[i]) ** 2) for i in range(18)]\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_title(f\"Similarity & MSE for video {key}\")\n",
    "        ax1.plot(cosine_sim, label='Cosine Similarity', marker='o', color='blue')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.set_ylabel('Cosine Similarity', color='blue')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(mse, label='MSE', marker='x', color='red')\n",
    "        ax2.set_ylabel('MSE', color='red')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.xlabel('Frame')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated audio feature 저장 함수\n",
    "\n",
    "def generate_and_save_audio_features(model, dataset, device, save_path):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, _, key = dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            pred = model(x).cpu().squeeze(0).numpy()  # (18, 128)\n",
    "            results[key] = pred\n",
    "    np.save(save_path, results)\n",
    "    print(f\"✅ Saved: {save_path} with {len(results)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioFeatureGenerator().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = sequence_mse_loss\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device, visualize=(epoch==10), num_samples=3)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test Evaluation\n",
    "\n",
    "def test(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "final_test_loss = test(model, test_loader, criterion, device)\n",
    "print(f\"\\n✅ Final Test Loss: {final_test_loss:.4f}\")\n",
    "\n",
    "def test_audio_similarity(model, dataset, device, num_samples=3):\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y, key = dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "            pred = model(x)\n",
    "\n",
    "            if len(samples) < num_samples:\n",
    "                samples.append((key, pred.squeeze(0).cpu().numpy(), y.squeeze(0).cpu().numpy()))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    visualize_audio_similarity_samples(samples)\n",
    "\n",
    "test_audio_similarity(model, test_loader.dataset, device, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated audio feature 저장 함수 호출\n",
    "\n",
    "generate_and_save_audio_features(model, train_loader.dataset, device, 'gen_audio_train.npy')\n",
    "generate_and_save_audio_features(model, val_loader.dataset, device, 'gen_audio_val.npy')\n",
    "generate_and_save_audio_features(model, test_loader.dataset, device, 'gen_audio_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM generated audio based Action Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Recognition Dataset\n",
    "\n",
    "class ActionRecognitionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, rgb_dict, flow_dict, audio_dict=None):\n",
    "        self.keys = list(rgb_dict.keys())\n",
    "        self.rgb_dict = rgb_dict\n",
    "        self.flow_dict = flow_dict\n",
    "        self.audio_dict = audio_dict  # None if audio is not used\n",
    "        self.use_audio = audio_dict is not None\n",
    "        self.label_map = {cls: i for i, cls in enumerate(sorted(set(k[0] for k in self.keys)))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        rgb = self.rgb_dict[key]      # (18, 768)\n",
    "        flow = self.flow_dict[key]    # (18, 768)\n",
    "        x = np.concatenate([rgb, flow], axis=-1)  # (18, 1536)\n",
    "\n",
    "        if self.use_audio:\n",
    "            audio = self.audio_dict[key]  # (18, 128)\n",
    "            x = np.concatenate([x, audio], axis=-1)  # (18, 1664)\n",
    "\n",
    "        label = self.label_map[key[0]]  # class_name → int label\n",
    "        return torch.tensor(x, dtype=torch.float32), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Recognition Dataset Loaders\n",
    "\n",
    "gen_audio_train = np.load(os.path.join(base_path, 'gen_audio_train.npy'), allow_pickle=True).item()\n",
    "gen_audio_val = np.load(os.path.join(base_path, 'gen_audio_val.npy'), allow_pickle=True).item()\n",
    "gen_audio_test = np.load(os.path.join(base_path, 'gen_audio_test.npy'), allow_pickle=True).item()\n",
    "\n",
    "# No Audio Datasets\n",
    "no_audio_train = ActionRecognitionDataset(rgb_train, flow_train)\n",
    "no_audio_val = ActionRecognitionDataset(rgb_val, flow_val)\n",
    "no_audio_test = ActionRecognitionDataset(rgb_test, flow_test)\n",
    "\n",
    "# With Generated Audio Datasets\n",
    "gen_audio_train_ds = ActionRecognitionDataset(rgb_train, flow_train, gen_audio_train)\n",
    "gen_audio_val_ds = ActionRecognitionDataset(rgb_val, flow_val, gen_audio_val)\n",
    "gen_audio_test_ds = ActionRecognitionDataset(rgb_test, flow_test, gen_audio_test)\n",
    "\n",
    "# DataLoaders\n",
    "no_audio_train_loader = DataLoader(no_audio_train, batch_size=16, shuffle=True)\n",
    "no_audio_val_loader = DataLoader(no_audio_val, batch_size=16)\n",
    "no_audio_test_loader = DataLoader(no_audio_test, batch_size=16)\n",
    "\n",
    "gen_audio_train_loader = DataLoader(gen_audio_train_ds, batch_size=16, shuffle=True)\n",
    "gen_audio_val_loader = DataLoader(gen_audio_val_ds, batch_size=16)\n",
    "gen_audio_test_loader = DataLoader(gen_audio_test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Recognition Classifier\n",
    "\n",
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=1, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # (B, 18, hidden_dim)\n",
    "        last_hidden = out[:, -1, :]  # (B, hidden_dim)\n",
    "        return self.fc(last_hidden)  # (B, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 함수\n",
    "\n",
    "def train_classifier(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_classifier(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (pred.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Audio (오디오 없이 비디오만 가지고 행동 인식)\n",
    "\n",
    "model = ActionClassifier(input_dim=1536).to(device)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train_loss, train_acc = train_classifier(model, no_audio_train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate_classifier(model, no_audio_val_loader, criterion, device)\n",
    "    print(f\"[Epoch {epoch}] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_classifier(model, no_audio_test_loader, criterion, device)\n",
    "print(f\"\\n✅ Final Test Accuracy (No Audio): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen Audio (LSTM 생성 오디오로 행동인식)\n",
    "\n",
    "model = ActionClassifier(input_dim=1664).to(device)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train_loss, train_acc = train_classifier(model, gen_audio_train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate_classifier(model, gen_audio_val_loader, criterion, device)\n",
    "    print(f\"[Epoch {epoch}] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_classifier(model, gen_audio_test_loader, criterion, device)\n",
    "print(f\"\\n✅ Final Test Accuracy (Gen Audio): {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT 오디오를 활용한 Action Recognition 평가용 Dataset 로딩\n",
    "\n",
    "audio_train_gt = np.load(os.path.join(base_path, \"audio_filtered_train.npy\"), allow_pickle=True).item()\n",
    "audio_val_gt = np.load(os.path.join(base_path, \"audio_filtered_val.npy\"), allow_pickle=True).item()\n",
    "audio_test_gt = np.load(os.path.join(base_path, \"audio_filtered_test.npy\"), allow_pickle=True).item()\n",
    "\n",
    "gt_audio_train_ds = ActionRecognitionDataset(rgb_train, flow_train, audio_train_gt)\n",
    "gt_audio_val_ds = ActionRecognitionDataset(rgb_val, flow_val, audio_val_gt)\n",
    "gt_audio_test_ds = ActionRecognitionDataset(rgb_test, flow_test, audio_test_gt)\n",
    "\n",
    "gt_audio_train_loader = DataLoader(gt_audio_train_ds, batch_size=16, shuffle=True)\n",
    "gt_audio_val_loader = DataLoader(gt_audio_val_ds, batch_size=16)\n",
    "gt_audio_test_loader = DataLoader(gt_audio_test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT 오디오로 ActionClassifier 학습 예시\n",
    "model = ActionClassifier(input_dim=1664).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "     train_loss, train_acc = train_classifier(model, gt_audio_train_loader, optimizer, criterion, device)\n",
    "     val_loss, val_acc = evaluate_classifier(model, gt_audio_val_loader, criterion, device)\n",
    "     print(f\"[Epoch {epoch}] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_classifier(model, gt_audio_test_loader, criterion, device)\n",
    "print(f\"✅ Final Test Accuracy (GT Audio): {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AST (Pretrained Multi-Label Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torchaudio librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification\n",
    "\n",
    "# 1. 모델 및 feature extractor 불러오기\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. waveform dictionary 불러오기\n",
    "waveform_dict = np.load(\"audio_waveform_test.npy\", allow_pickle=True).item()\n",
    "\n",
    "# 3. 결과 저장용 딕셔너리\n",
    "pred_dict = {}\n",
    "skipped_keys = []\n",
    "\n",
    "# 4. 예측 수행\n",
    "for key, waveform in tqdm(waveform_dict.items()):\n",
    "    try:\n",
    "        # 1D numpy array로 squeeze\n",
    "        waveform = np.array(waveform).squeeze()\n",
    "\n",
    "        # 너무 짧은 waveform은 스킵\n",
    "        if len(waveform) < 400:\n",
    "            skipped_keys.append((key, \"Too short\"))\n",
    "            continue\n",
    "\n",
    "        # Feature extraction 및 모델 예측\n",
    "        inputs = feature_extractor(\n",
    "            waveform,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.sigmoid(outputs.logits).squeeze().numpy()  # [527]\n",
    "            pred_dict[key] = probs\n",
    "\n",
    "    except Exception as e:\n",
    "        skipped_keys.append((key, str(e)))\n",
    "        continue\n",
    "\n",
    "# 5. 결과 저장\n",
    "np.save(\"ast_pred_audio_test.npy\", pred_dict)\n",
    "np.save(\"ast_pred_audio_test_skipped.npy\", np.array(skipped_keys, dtype=object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스-의미 매핑\n",
    "from transformers import ASTForAudioClassification\n",
    "import numpy as np\n",
    "\n",
    "# 모델 불러오기\n",
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "id2label = model.config.id2label  # {int: str}\n",
    "\n",
    "# 예측 결과 불러오기\n",
    "pred_dict = np.load(\"ast_pred_audio_test.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Top-k 예측 결과를 이름으로 매핑해서 저장\n",
    "topk = 5\n",
    "mapped_dict = {}\n",
    "\n",
    "for key, probs in pred_dict.items():\n",
    "    topk_indices = probs.argsort()[-topk:][::-1]  # 큰 값부터 정렬\n",
    "    topk_labels = [id2label[idx] for idx in topk_indices]\n",
    "    topk_scores = [float(probs[idx]) for idx in topk_indices]\n",
    "\n",
    "    mapped_dict[key] = {\n",
    "        \"topk_labels\": topk_labels,\n",
    "        \"topk_scores\": topk_scores\n",
    "    }\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "json_mapped_dict = {\n",
    "    f\"{k[0]}|{k[1]}\": v for k, v in mapped_dict.items()\n",
    "}\n",
    "\n",
    "with open(\"ast_pred_test_top5.json\", \"w\") as f:\n",
    "    json.dump(json_mapped_dict, f, indent=2)\n",
    "\n",
    "np.save(\"ast_pred_test_top5.npy\", mapped_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU (Intersection over Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. 파일 불러오기\n",
    "with open(\"data/pred_test.json\", \"r\") as f:\n",
    "    pred_dict = json.load(f)\n",
    "\n",
    "savld_df = pd.read_csv(\"data/SVD.csv\")\n",
    "\n",
    "# 2. SAVLD dictionary 생성: {class_name: [audio_label1, audio_label2, ...]}\n",
    "savld_dict = {}\n",
    "for _, row in savld_df.iterrows():\n",
    "    class_name = row[\"Video Label\"].strip()\n",
    "    audio_labels = [l.strip() for l in row[\"Audio Labels\"].split(\";\")]\n",
    "    savld_dict[class_name] = audio_labels\n",
    "\n",
    "# 3. 모든 라벨 집합 수집\n",
    "all_labels = set()\n",
    "for pred in pred_dict.values():\n",
    "    all_labels.update(pred[\"topk_labels\"])\n",
    "for gt_labels in savld_dict.values():\n",
    "    all_labels.update(gt_labels)\n",
    "all_labels = sorted(all_labels)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=all_labels)\n",
    "mlb.fit([all_labels])  # fit with full label set\n",
    "\n",
    "# 4. 클래스별 IOU 계산\n",
    "class_ious = defaultdict(list)\n",
    "\n",
    "for key, pred_info in pred_dict.items():\n",
    "    if \"|\" not in key:\n",
    "        continue\n",
    "    class_name, _ = key.split(\"|\", 1)\n",
    "    if class_name not in savld_dict:\n",
    "        continue\n",
    "\n",
    "    pred_labels = pred_info[\"topk_labels\"]\n",
    "    gt_labels = savld_dict[class_name]\n",
    "\n",
    "    pred_vec = mlb.transform([pred_labels])[0]\n",
    "    gt_vec = mlb.transform([gt_labels])[0]\n",
    "\n",
    "    iou = jaccard_score(gt_vec, pred_vec)\n",
    "    class_ious[class_name].append(iou)\n",
    "\n",
    "# 5. 클래스별 평균 IOU 계산\n",
    "class_avg_ious = {\n",
    "    cls: np.mean(iou_list) for cls, iou_list in class_ious.items()\n",
    "}\n",
    "\n",
    "# 6. 출력\n",
    "print(\"[INFO] Class-wise average IOU:\")\n",
    "for cls, iou in sorted(class_avg_ious.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cls:30s} : {iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# base 경로 설정\n",
    "base_path = \"data\"  # 필요 시 절대경로로 수정\n",
    "\n",
    "# 필터링 기준 클래스\n",
    "selected_classes = [\n",
    "    'adult+female+singing',\n",
    "    'adult+female+speaking',\n",
    "    'adult+male+speaking',\n",
    "    'adult+male+singing',\n",
    "    'applauding'\n",
    "]\n",
    "\n",
    "# 파일명 리스트\n",
    "modalities = [\"audio\", \"rgb\", \"flow\"]\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for modality in modalities:\n",
    "    for split in splits:\n",
    "        # 원본 파일 로딩\n",
    "        fname = f\"{modality}_filtered_{split}.npy\"\n",
    "        full_path = os.path.join(base_path, fname)\n",
    "        data = np.load(full_path, allow_pickle=True).item()\n",
    "\n",
    "        # 클래스 기준 필터링\n",
    "        filtered_data = {\n",
    "            k: v for k, v in data.items() if k[0] in selected_classes\n",
    "        }\n",
    "\n",
    "        # 새 파일 저장\n",
    "        new_fname = f\"{modality}_filtered_{split}_selected.npy\"\n",
    "        new_path = os.path.join(base_path, new_fname)\n",
    "        np.save(new_path, filtered_data)\n",
    "        print(f\"[SAVED] {new_path} ({len(filtered_data)} samples)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM audio feature generation (filtered.ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Preprocessed .npy Files\n",
    "\n",
    "base_path = os.path.join(os.path.dirname(__file__), 'data') if '__file__' in globals() else 'data/'\n",
    "\n",
    "audio_train = np.load(os.path.join(base_path, \"audio_filtered_train_selected.npy\"), allow_pickle=True).item()\n",
    "audio_val   = np.load(os.path.join(base_path, \"audio_filtered_val_selected.npy\"), allow_pickle=True).item()\n",
    "audio_test  = np.load(os.path.join(base_path, \"audio_filtered_test_selected.npy\"), allow_pickle=True).item()\n",
    "\n",
    "rgb_train = np.load(os.path.join(base_path, \"rgb_filtered_train_selected.npy\"), allow_pickle=True).item()\n",
    "rgb_val   = np.load(os.path.join(base_path, \"rgb_filtered_val_selected.npy\"), allow_pickle=True).item()\n",
    "rgb_test  = np.load(os.path.join(base_path, \"rgb_filtered_test_selected.npy\"), allow_pickle=True).item()\n",
    "\n",
    "flow_train = np.load(os.path.join(base_path, \"flow_filtered_train_selected.npy\"), allow_pickle=True).item()\n",
    "flow_val   = np.load(os.path.join(base_path, \"flow_filtered_val_selected.npy\"), allow_pickle=True).item()\n",
    "flow_test  = np.load(os.path.join(base_path, \"flow_filtered_test_selected.npy\"), allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "\n",
    "class VideoToAudioDataset(Dataset):\n",
    "    def __init__(self, rgb_dict, flow_dict, audio_dict):\n",
    "        self.keys = list(audio_dict.keys())\n",
    "        self.rgb_dict = rgb_dict\n",
    "        self.flow_dict = flow_dict\n",
    "        self.audio_dict = audio_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        rgb_feat = self.rgb_dict[key]      # (18, 768)\n",
    "        flow_feat = self.flow_dict[key]    # (18, 768)\n",
    "        x = np.concatenate([rgb_feat, flow_feat], axis=-1)  # (18, 1536)\n",
    "        y = self.audio_dict[key]           # (18, 128)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 \n",
    "\n",
    "train_loader = DataLoader(VideoToAudioDataset(rgb_train, flow_train, audio_train), batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(VideoToAudioDataset(rgb_val, flow_val, audio_val), batch_size=4)\n",
    "test_loader = DataLoader(VideoToAudioDataset(rgb_test, flow_test, audio_test), batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-based generator \n",
    "\n",
    "class AudioFeatureGenerator(nn.Module):\n",
    "    def __init__(self, input_dim=1536, hidden_dim=512, num_layers=2, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # (B, 18, H)\n",
    "        return self.fc(out)    # (B, 18, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Validation Functions\n",
    "\n",
    "def sequence_mse_loss(pred, target):\n",
    "    return ((pred - target) ** 2).mean()\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y, _ in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)  # (B, 18, 218)\n",
    "        loss = criterion(pred, y)  # element-wise comparison\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, criterion, device, visualize=False, num_samples=3):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(loader.dataset)):\n",
    "            x, y, key = loader.dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if visualize and len(samples) < num_samples:\n",
    "                samples.append((key, pred.squeeze(0).cpu().numpy(), y.squeeze(0).cpu().numpy()))\n",
    "\n",
    "    if visualize:\n",
    "        visualize_audio_similarity_samples(samples)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def visualize_audio_similarity_samples(samples):\n",
    "    for key, pred, gt in samples:\n",
    "        cosine_sim = [cosine_similarity(pred[i:i+1], gt[i:i+1])[0, 0] for i in range(18)]\n",
    "        mse = [np.mean((pred[i] - gt[i]) ** 2) for i in range(18)]\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_title(f\"Similarity & MSE for video {key}\")\n",
    "        ax1.plot(cosine_sim, label='Cosine Similarity', marker='o', color='blue')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.set_ylabel('Cosine Similarity', color='blue')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(mse, label='MSE', marker='x', color='red')\n",
    "        ax2.set_ylabel('MSE', color='red')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.xlabel('Frame')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated audio feature 저장 함수\n",
    "\n",
    "def generate_and_save_audio_features(model, dataset, device, save_path):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, _, key = dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            pred = model(x).cpu().squeeze(0).numpy()  # (18, 128)\n",
    "            results[key] = pred\n",
    "    np.save(save_path, results)\n",
    "    print(f\"✅ Saved: {save_path} with {len(results)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioFeatureGenerator().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = sequence_mse_loss\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device, visualize=(epoch==10), num_samples=3)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test Evaluation\n",
    "\n",
    "def test(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "final_test_loss = test(model, test_loader, criterion, device)\n",
    "print(f\"\\n✅ Final Test Loss: {final_test_loss:.4f}\")\n",
    "\n",
    "def test_audio_similarity(model, dataset, device, num_samples=3):\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y, key = dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "            pred = model(x)\n",
    "\n",
    "            if len(samples) < num_samples:\n",
    "                samples.append((key, pred.squeeze(0).cpu().numpy(), y.squeeze(0).cpu().numpy()))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    visualize_audio_similarity_samples(samples)\n",
    "\n",
    "test_audio_similarity(model, test_loader.dataset, device, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM generated audio based Action Recognition (filtered.ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated audio feature 저장 함수 호출 (IOU-filtered version)\n",
    "generate_and_save_audio_features(model, train_loader.dataset, device, 'gen_audio_train_selected.npy')\n",
    "generate_and_save_audio_features(model, val_loader.dataset, device, 'gen_audio_val_selected.npy')\n",
    "generate_and_save_audio_features(model, test_loader.dataset, device, 'gen_audio_test_selected.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Recognition Dataset\n",
    "\n",
    "class ActionRecognitionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, rgb_dict, flow_dict, audio_dict=None):\n",
    "        self.keys = list(rgb_dict.keys())\n",
    "        self.rgb_dict = rgb_dict\n",
    "        self.flow_dict = flow_dict\n",
    "        self.audio_dict = audio_dict  # None if audio is not used\n",
    "        self.use_audio = audio_dict is not None\n",
    "        self.label_map = {cls: i for i, cls in enumerate(sorted(set(k[0] for k in self.keys)))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        rgb = self.rgb_dict[key]      # (18, 768)\n",
    "        flow = self.flow_dict[key]    # (18, 768)\n",
    "        x = np.concatenate([rgb, flow], axis=-1)  # (18, 1536)\n",
    "\n",
    "        if self.use_audio:\n",
    "            audio = self.audio_dict[key]  # (18, 128)\n",
    "            x = np.concatenate([x, audio], axis=-1)  # (18, 1664)\n",
    "\n",
    "        label = self.label_map[key[0]]  # class_name → int label\n",
    "        return torch.tensor(x, dtype=torch.float32), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Recognition Dataset Loaders\n",
    "\n",
    "gen_audio_train = np.load(os.path.join(base_path, 'gen_audio_train_selected.npy'), allow_pickle=True).item()\n",
    "gen_audio_val = np.load(os.path.join(base_path, 'gen_audio_val_selected.npy'), allow_pickle=True).item()\n",
    "gen_audio_test = np.load(os.path.join(base_path, 'gen_audio_test_selected.npy'), allow_pickle=True).item()\n",
    "\n",
    "# No Audio Datasets\n",
    "no_audio_train = ActionRecognitionDataset(rgb_train, flow_train)\n",
    "no_audio_val = ActionRecognitionDataset(rgb_val, flow_val)\n",
    "no_audio_test = ActionRecognitionDataset(rgb_test, flow_test)\n",
    "\n",
    "# With Generated Audio Datasets\n",
    "gen_audio_train_ds = ActionRecognitionDataset(rgb_train, flow_train, gen_audio_train)\n",
    "gen_audio_val_ds = ActionRecognitionDataset(rgb_val, flow_val, gen_audio_val)\n",
    "gen_audio_test_ds = ActionRecognitionDataset(rgb_test, flow_test, gen_audio_test)\n",
    "\n",
    "# DataLoaders\n",
    "no_audio_train_loader = DataLoader(no_audio_train, batch_size=16, shuffle=True)\n",
    "no_audio_val_loader = DataLoader(no_audio_val, batch_size=16)\n",
    "no_audio_test_loader = DataLoader(no_audio_test, batch_size=16)\n",
    "\n",
    "gen_audio_train_loader = DataLoader(gen_audio_train_ds, batch_size=16, shuffle=True)\n",
    "gen_audio_val_loader = DataLoader(gen_audio_val_ds, batch_size=16)\n",
    "gen_audio_test_loader = DataLoader(gen_audio_test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Recognition Classifier\n",
    "\n",
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=1, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # (B, 18, hidden_dim)\n",
    "        last_hidden = out[:, -1, :]  # (B, hidden_dim)\n",
    "        return self.fc(last_hidden)  # (B, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 함수\n",
    "\n",
    "def train_classifier(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_classifier(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (pred.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen Audio\n",
    "\n",
    "model = ActionClassifier(input_dim=1664).to(device)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train_loss, train_acc = train_classifier(model, gen_audio_train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate_classifier(model, gen_audio_val_loader, criterion, device)\n",
    "    print(f\"[Epoch {epoch}] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_classifier(model, gen_audio_test_loader, criterion, device)\n",
    "print(f\"\\n✅ Final Test Accuracy (Gen Audio): {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
