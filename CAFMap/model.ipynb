{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **representative frame extractor + caption generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "def load_frames(folder_path):\n",
    "    image_files = sorted([f for f in os.listdir(folder_path) if f.endswith((\".jpg\", \".png\"))])\n",
    "    image_paths = [os.path.join(folder_path, f) for f in image_files]\n",
    "    frames = [cv2.imread(p) for p in image_paths]\n",
    "    return frames\n",
    "\n",
    "def representative_extractor(frames, threshold_ratio=1.2):\n",
    "    optical_flow = cv2.DISOpticalFlow_create(cv2.DISOPTICAL_FLOW_PRESET_FAST)\n",
    "    magnitudes = []\n",
    "    for i in range(len(frames) - 1):\n",
    "        prev = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "        next = cv2.cvtColor(frames[i + 1], cv2.COLOR_BGR2GRAY)\n",
    "        flow = optical_flow.calc(prev, next, None)\n",
    "        magnitude = np.linalg.norm(flow, axis=2).mean()\n",
    "        magnitudes.append(magnitude)\n",
    "\n",
    "    avg = np.mean(magnitudes)\n",
    "    rep_indices = [i + 1 for i, m in enumerate(magnitudes) if m >= threshold_ratio * avg]\n",
    "    return rep_indices\n",
    "\n",
    "def load_caption(frames, indices, processor, model, device):\n",
    "    results = []\n",
    "    for idx in indices:\n",
    "        image = cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(image)\n",
    "        inputs = processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs)\n",
    "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "        results.append({\"frame\": idx, \"caption\": caption})\n",
    "    return results\n",
    "\n",
    "def representative_caption_generator(root_dir, processor, model, device, threshold_ratio=1.2, output_path=\"captions.json\"):\n",
    "    result_dict = {}\n",
    "\n",
    "    video_ids = sorted(os.listdir(root_dir))\n",
    "    for video_id in tqdm(video_ids):\n",
    "        folder_path = os.path.join(root_dir, video_id)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        frames = load_frames(folder_path)\n",
    "        if len(frames) != 18:\n",
    "            print(f\"[WARNING] {video_id}: expected 18 frames, found {len(frames)}\")\n",
    "            continue\n",
    "\n",
    "        rep_indices = representative_extractor(frames, threshold_ratio)\n",
    "        captions = load_caption(frames, rep_indices, processor, model, device)\n",
    "        result_dict[video_id] = captions\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(result_dict, f, indent=2)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ ÎåÄÌëú ÌîÑÎ†àÏûÑ & Ï∫°ÏÖò Ï∂îÏ∂ú**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_all(root_dir, target_classes, output_dir, processor, model, device, threshold_ratio=1.2):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for split in [\"training\", \"validation\"]:\n",
    "        print(f\"\\n[INFO] Processing split: {split}\")\n",
    "        for class_name in tqdm(target_classes, desc=f\"{split} split\"):\n",
    "            class_path = os.path.join(root_dir, split, class_name, \"18frames\")\n",
    "            if not os.path.isdir(class_path):\n",
    "                print(f\"[SKIP] {class_path} not found.\")\n",
    "                continue\n",
    "            \n",
    "            result_dict = {}\n",
    "            for video_id in sorted(os.listdir(class_path)):\n",
    "                video_folder = os.path.join(class_path, video_id)\n",
    "                if not os.path.isdir(video_folder): continue\n",
    "                \n",
    "                frames = load_frames(video_folder)\n",
    "                if len(frames) != 18:\n",
    "                    print(f\"[WARNING] {video_id}: expected 18 frames, found {len(frames)}\")\n",
    "                    continue\n",
    "                \n",
    "                rep_indices = representative_extractor(frames, threshold_ratio)\n",
    "                captions = load_caption(frames, rep_indices, processor, model, device)\n",
    "                result_dict[video_id] = captions\n",
    "            save_path = os.path.join(output_dir, f\"{split}_{class_name}_captions.json\")\n",
    "            with open(save_path, \"w\") as f:\n",
    "                json.dump(result_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "ÌÉÄÍ≤ü classÎäî Ïã§Ìóò Î≤îÏúÑÏóê ÎßûÏ∂∞ ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "target_classes = [\n",
    "    \"adult+female+singing\", \"adult+female+speaking\", \"adult+male+singing\",\n",
    "    \"adult+male+speaking\", \"applauding\", \"ascending\", \"asking\", \"assembling\",\n",
    "    \"autographing\", \"baking\", \"balancing\", \"barbecuing\", \"barking\", \"bending\",\n",
    "    \"bicycling\", \"biting\", \"blowing\", \"boarding\", \"boating\", \"boiling\"\n",
    "    ]\n",
    "\n",
    "root_dir = \"D:/RGB\"\n",
    "output_dir = \"D:/caption_json_output\"\n",
    "process_all(root_dir, target_classes, output_dir, processor, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AudioSet .tfrecord Ï≤òÎ¶¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .tfrecord ÌååÏùº ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    context_features = {\n",
    "        \"video_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"labels\": tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "\n",
    "    sequence_features = {\n",
    "        \"audio_embedding\": tf.io.FixedLenSequenceFeature([], dtype=tf.string),  # [] not [128]\n",
    "    }\n",
    "\n",
    "    contexts, sequences = tf.io.parse_single_sequence_example(\n",
    "        serialized=example_proto,\n",
    "        context_features=context_features,\n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "\n",
    "    audio_bytes = sequences[\"audio_embedding\"]  \n",
    "    audio_decoded = tf.map_fn(\n",
    "        lambda x: tf.cast(tf.io.decode_raw(x, tf.uint8), tf.float32),\n",
    "        audio_bytes,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    label_ids = tf.cast(tf.sparse.to_dense(contexts[\"labels\"]), tf.int64)\n",
    "    label_vector = tf.reduce_sum(tf.one_hot(label_ids, depth=527), axis=0)\n",
    "\n",
    "    return audio_decoded, label_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfrecord >> npy ÎîïÏÖîÎÑàÎ¶¨ Î≥ÄÌôò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_label_mapping(label_csv_path):\n",
    "    df = pd.read_csv(label_csv_path)\n",
    "    mid_to_name = dict(zip(df[\"mid\"], df[\"display_name\"]))\n",
    "    return mid_to_name\n",
    "\n",
    "def load_segment_labels(segment_csv_path):\n",
    "    with open(segment_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    lines = lines[3:]\n",
    "    ytid_to_mids = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            parts = line.strip().split(\",\", 3) \n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            ytid = parts[0].strip()\n",
    "            labels = parts[3].strip().strip('\"')\n",
    "            ytid_to_mids[ytid] = labels\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Failed to parse line: {line} -- {e}\")\n",
    "            continue\n",
    "\n",
    "    return ytid_to_mids\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    context_features = {\n",
    "        \"video_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"labels\": tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    sequence_features = {\n",
    "        \"audio_embedding\": tf.io.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "    }\n",
    "\n",
    "    contexts, features = tf.io.parse_single_sequence_example(\n",
    "        example_proto,\n",
    "        context_features=context_features,\n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "\n",
    "    audio_bytes = tf.io.decode_raw(features[\"audio_embedding\"], tf.uint8)\n",
    "    audio_float = tf.cast(audio_bytes, tf.float32)\n",
    "    audio_features = tf.reshape(audio_float, [-1, 128])  # shape: [10, 128]\n",
    "\n",
    "    video_id = contexts[\"video_id\"]\n",
    "    return video_id, audio_features\n",
    "\n",
    "def extract_and_save_audioset_features(tfrecord_dir, label_csv_path, segment_csv_path, output_path, limit=None):\n",
    "    print(\"Loading label mappings...\")\n",
    "    mid_to_name = load_label_mapping(label_csv_path)\n",
    "    ytid_to_mids = load_segment_labels(segment_csv_path)\n",
    "\n",
    "    feature_dict = {}\n",
    "    tfrecord_files = [f for f in os.listdir(tfrecord_dir) if f.endswith(\".tfrecord\")]\n",
    "    tfrecord_files.sort()\n",
    "\n",
    "    for filename in tqdm(tfrecord_files, desc=\"Processing TFRecords\"):\n",
    "        dataset = tf.data.TFRecordDataset(os.path.join(tfrecord_dir, filename))\n",
    "        for raw_record in dataset:\n",
    "            try:\n",
    "                video_id_tensor, audio_feat = _parse_function(raw_record)\n",
    "                video_id = video_id_tensor.numpy().decode(\"utf-8\")\n",
    "\n",
    "                mids_str = ytid_to_mids.get(video_id, \"\")\n",
    "                mids = mids_str.split(\",\") if mids_str else []\n",
    "                label_names = [mid_to_name.get(mid.strip(), mid.strip()) for mid in mids]\n",
    "\n",
    "                feature_dict[video_id] = {\n",
    "                    \"features\": audio_feat.numpy(),  # shape: [10, 128]\n",
    "                    \"labels\": label_names\n",
    "                }\n",
    "\n",
    "                if limit and len(feature_dict) >= limit:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename}: {e}\")\n",
    "        if limit and len(feature_dict) >= limit:\n",
    "            break\n",
    "\n",
    "    np.save(output_path, feature_dict)\n",
    "    print(f\"Saved {len(feature_dict)} samples to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "extract_and_save_audioset_features(\n",
    "    tfrecord_dir=r\"C:\\Users\\swu\\Desktop\\audioset_v1_embeddings\\bal_train\",  \n",
    "    label_csv_path=r\"C:\\Users\\swu\\Desktop\\class_labels_indices.csv\", \n",
    "    segment_csv_path=r\"C:\\Users\\swu\\Desktop\\balanced_train_segments.csv\", \n",
    "    output_path=r\"C:\\Users\\swu\\Desktop\\audioset_v1_embeddings\\audioset_balanced_train.npy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "npy_path = r\"C:\\Users\\swu\\Desktop\\audioset_v1_embeddings\\audioset_balanced_train.npy\"\n",
    "data = np.load(npy_path, allow_pickle=True).item() \n",
    "\n",
    "for i, (vid, info) in enumerate(data.items()):\n",
    "    print(f\"\\nüîπ Sample {i+1}\")\n",
    "    print(\"‚ñ∂ Video ID:\", vid)\n",
    "    print(\"‚ñ∂ Audio Feature Shape:\", info[\"features\"])\n",
    "    print(\"‚ñ∂ Labels:\", info[\"labels\"])\n",
    "    if i >= 10:\n",
    "        break  # Ï≤òÏùå 5Í∞úÍπåÏßÄÎßå Ï∂úÎ†•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AudioSet class Î≥Ñ ÌèâÍ∑† Ïò§ÎîîÏò§ ÌîºÏ≤ò Í≥ÑÏÇ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "data = np.load(r\"C:\\Users\\swu\\Desktop\\audioset_v1_embeddings\\audioset_balanced_train.npy\", allow_pickle=True).item()\n",
    "class_to_features = defaultdict(list)\n",
    "\n",
    "for video_id, sample in tqdm(data.items(), desc=\"Aggregating features\"):\n",
    "    features = sample[\"features\"]  # shape: [T, 128]\n",
    "    mean_feat = np.mean(features, axis=0)  # [128]\n",
    "\n",
    "    for label in sample[\"labels\"]:\n",
    "        class_to_features[label].append(mean_feat)\n",
    "\n",
    "class_to_avg = {}\n",
    "\n",
    "for label, feats in class_to_features.items():\n",
    "    class_to_avg[label] = np.mean(feats, axis=0)  # shape: [128]\n",
    "\n",
    "np.save(r\"C:\\Users\\swu\\Desktop\\audioset_v1_embeddings\\audioset_class_embeddings.npy\", class_to_avg)\n",
    "print(\"ÌÅ¥ÎûòÏä§Î≥Ñ ÌèâÍ∑† Ïò§ÎîîÏò§ ÌîºÏ≤ò Ï†ÄÏû• ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "class_avg = np.load(r\"C:\\Users\\swu\\Desktop\\audioset_v1_embeddings\\audioset_class_embeddings.npy\", allow_pickle=True).item()\n",
    "print(f\"Ï¥ù ÌÅ¥ÎûòÏä§ Ïàò: {len(class_avg)}\")\n",
    "\n",
    "shapes = set(v.shape for v in class_avg.values())\n",
    "print(f\"ÌèâÍ∑† ÌîºÏ≤ò Î≤°ÌÑ∞ shape: {shapes}\")\n",
    "\n",
    "for i, (label, vec) in enumerate(class_avg.items()):\n",
    "    print(f\"\\nÌÅ¥ÎûòÏä§: {label}\")\n",
    "    print(f\"ÌîºÏ≤ò Î≤°ÌÑ∞ ÏùºÎ∂Ä: {vec[:10]}\")\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Î©îÏù∏ ÌïôÏäµ ÎÑ§Ìä∏ÏõåÌÅ¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÎåÄÌëú ÌîÑÎ†àÏûÑ Ï∫°ÏÖò Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_all_training_captions(caption_dir):\n",
    "    \"\"\"\n",
    "    caption_dir: str - \"D:/caption_json_output\"\n",
    "    returns: dict - { (class_name, video_id): [caption1, caption2, ...] }\n",
    "    \"\"\"\n",
    "    caption_dict = {}\n",
    "\n",
    "    for fname in os.listdir(caption_dir): \n",
    "        if fname.startswith(\"training_\") and fname.endswith(\"_captions.json\"):\n",
    "            class_name = fname[len(\"training_\"):-len(\"_captions.json\")]\n",
    "            file_path = os.path.join(caption_dir, fname)\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            for video_id, captions in data.items():\n",
    "                caption_dict[(class_name, video_id)] = captions\n",
    "    return caption_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ï∫°ÏÖò - ÎùºÎ≤®&ÌîºÏ≤ò Îß§Ìïë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def load_label_feature_dict(npy_path):\n",
    "    return np.load(npy_path, allow_pickle=True).item()\n",
    "\n",
    "def map_caption_to_features(caption, label_feat_dict, top_k=5):\n",
    "    label_names = list(label_feat_dict.keys())\n",
    "\n",
    "    caption_emb = model.encode([caption]) \n",
    "    label_embs = model.encode(label_names)\n",
    "\n",
    "    similarities = cosine_similarity(caption_emb, label_embs)[0]\n",
    "    topk_indices = similarities.argsort()[::-1][:top_k]\n",
    "    topk_scores = similarities[topk_indices]\n",
    "    sim_mean = np.mean(topk_scores)\n",
    "\n",
    "    selected = []\n",
    "    for i in topk_indices:\n",
    "        if similarities[i] >= sim_mean:\n",
    "            label = label_names[i]\n",
    "            feature = label_feat_dict[label]\n",
    "            selected.append((label, feature))\n",
    "\n",
    "    selected_labels = [x[0] for x in selected]\n",
    "    selected_features = [x[1] for x in selected]\n",
    "    return selected_labels, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM-based Generator ÌååÏù¥ÌîÑÎùºÏù∏**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AudioFeatureGenerator(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=512, num_layers=2, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "class AudioGenerationDataset(Dataset):\n",
    "    def __init__(self, keys, gt_audio_dict, condition_dict, sequence_len=18, noise_dim=128):\n",
    "        self.keys = keys\n",
    "        self.gt_audio = gt_audio_dict\n",
    "        self.condition = condition_dict\n",
    "        self.seq_len = sequence_len\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        gt = self.gt_audio[key]\n",
    "        condition_mask = self.condition[key]['mask']    \n",
    "        condition_vectors = self.condition[key]['features']\n",
    "\n",
    "        noise = np.random.normal(0, 1, (self.seq_len, self.noise_dim)).astype(np.float32)\n",
    "        input_seq = np.where(np.expand_dims(condition_mask, axis=1), condition_vectors, noise)\n",
    "        input_seq = torch.tensor(input_seq, dtype=torch.float32)\n",
    "        gt = torch.tensor(gt, dtype=torch.float32)\n",
    "\n",
    "        return input_seq, gt, key\n",
    "\n",
    "def sequence_mse_loss(pred, target):\n",
    "    return ((pred - target) ** 2).mean()\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y, _ in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, criterion, device, visualize=False, num_samples=3):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(loader.dataset)):\n",
    "            x, y, key = loader.dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if visualize and len(samples) < num_samples:\n",
    "                samples.append((key, pred.squeeze(0).cpu().numpy(), y.squeeze(0).cpu().numpy()))\n",
    "\n",
    "    if visualize:\n",
    "        visualize_audio_similarity_samples(samples)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def visualize_audio_similarity_samples(samples):\n",
    "    for key, pred, gt in samples:\n",
    "        cosine_sim = [cosine_similarity(pred[i:i+1], gt[i:i+1])[0, 0] for i in range(18)]\n",
    "        mse = [np.mean((pred[i] - gt[i]) ** 2) for i in range(18)]\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_title(f\"Similarity & MSE for video {key}\")\n",
    "        ax1.plot(cosine_sim, label='Cosine Similarity', marker='o', color='blue')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.set_ylabel('Cosine Similarity', color='blue')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(mse, label='MSE', marker='x', color='red')\n",
    "        ax2.set_ylabel('MSE', color='red')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.xlabel('Frame')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "condition_dict Îß§Ìïë  \n",
    "-> Í∞Å ÎπÑÎîîÏò§Ïóê ÎåÄÌïú Ï°∞Í±¥ ÏûÖÎ†•ÏùÑ Îß§Ìïë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "caption_dict = load_all_training_captions(r\"D:/caption_json_output\")\n",
    "label_feat_dict = load_label_feature_dict(\n",
    "    r\"C:\\Users\\swu\\Desktop\\audioset_v1_embeddings\\audioset_class_embeddings.npy\"\n",
    ")\n",
    "label_names = list(label_feat_dict.keys())\n",
    "caption_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "label_embs = caption_encoder.encode(label_names)\n",
    "\n",
    "def map_caption_to_features_fast(caption, label_names, label_embs, label_feat_dict, top_k=5):\n",
    "    caption_emb = caption_encoder.encode([caption]) \n",
    "    similarities = cosine_similarity(caption_emb, label_embs)[0]\n",
    "    \n",
    "    topk_indices = similarities.argsort()[::-1][:top_k]\n",
    "    topk_scores = similarities[topk_indices]\n",
    "    sim_mean = np.mean(topk_scores)\n",
    "    selected_feats = [\n",
    "        label_feat_dict[label_names[i]]\n",
    "        for i in topk_indices if similarities[i] >= sim_mean\n",
    "    ]\n",
    "    return selected_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_dict = {}\n",
    "\n",
    "for key, captions in tqdm(caption_dict.items(), desc=\"Generating conditions\"):\n",
    "    mask = np.zeros(18, dtype=bool)\n",
    "    features = np.zeros((18, 128), dtype=np.float32)\n",
    "    \n",
    "    for caption_info in captions:\n",
    "        frame_idx = caption_info[\"frame\"]\n",
    "        caption_text = caption_info[\"caption\"]\n",
    "\n",
    "        selected_feats = map_caption_to_features_fast(\n",
    "            caption_text, label_names, label_embs, label_feat_dict, top_k=5\n",
    "        )\n",
    "\n",
    "        if selected_feats:\n",
    "            avg_feat = np.mean(selected_feats, axis=0)\n",
    "            features[frame_idx] = avg_feat\n",
    "            mask[frame_idx] = True\n",
    "\n",
    "    condition_dict[key] = {\n",
    "        \"mask\": mask,        \n",
    "        \"features\": features \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "gt_train = np.load(r\"D:\\Audio-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\audio_filtered_train.npy\", allow_pickle=True).item()\n",
    "gt_val   = np.load(r\"D:\\Audio-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\audio_filtered_val.npy\", allow_pickle=True).item()\n",
    "train_keys = list(set(gt_train.keys()) & set(condition_dict.keys()))\n",
    "val_keys   = list(set(gt_val.keys()) & set(condition_dict.keys()))\n",
    "\n",
    "print(f\"[INFO] Train samples: {len(train_keys)}\")\n",
    "print(f\"[INFO] Validation samples: {len(val_keys)}\")\n",
    "\n",
    "train_dataset = AudioGenerationDataset(train_keys, gt_train, condition_dict)\n",
    "val_dataset   = AudioGenerationDataset(val_keys, gt_val, condition_dict)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"[INFO] # of train batches: {len(train_loader)}\")\n",
    "print(f\"[INFO] # of val batches: {len(val_loader)}\")\n",
    "\n",
    "sample_x, sample_y, sample_key = next(iter(train_loader))\n",
    "print(f\"[INFO] Sample input shape: {sample_x.shape}\")  # (B, 18, 128)\n",
    "print(f\"[INFO] Sample target shape: {sample_y.shape}\")  # (B, 18, 128)\n",
    "print(f\"[INFO] First sample key: {sample_key[0]}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioFeatureGenerator(input_dim=128, hidden_dim=512, output_dim=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = sequence_mse_loss\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device, visualize=False)\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_and_save_audio_features(model, dataset, device, save_path):\n",
    "    model.eval()\n",
    "    generated_dict = {}\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Generating audio features\"):\n",
    "        input_seq, _, key = dataset[i]\n",
    "        input_seq = input_seq.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "        generated_dict[key] = output.squeeze(0).cpu().numpy()\n",
    "\n",
    "    np.save(save_path, generated_dict)\n",
    "    print(f\"Saved {len(generated_dict)} generated features to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "\n",
    "generate_and_save_audio_features(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    device=device,\n",
    "    save_path=r\"D:\\Gen-Audio\\gen_audio_v1_train.npy\"\n",
    ")\n",
    "\n",
    "generate_and_save_audio_features(\n",
    "    model=model,\n",
    "    dataset=val_dataset,\n",
    "    device=device,\n",
    "    save_path=r\"D:\\Gen-Audio\\gen_audio_v1_val.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Action Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class ActionClassificationDataset(Dataset):\n",
    "    def __init__(self, csv_path, rgb_dict, flow_dict, audio_dict, label_map, split_keys=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.rgb = rgb_dict\n",
    "        self.flow = flow_dict\n",
    "        self.audio = audio_dict\n",
    "        self.label_map = label_map\n",
    "        self.keys = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            key = (row['class_name'], row['video_id'])\n",
    "            if key in rgb_dict and key in flow_dict and key in audio_dict and row['class_name'] in label_map:\n",
    "                if split_keys is None or key in split_keys:\n",
    "                    self.keys.append(key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        rgb = self.rgb[key]     # [18, 768]\n",
    "        flow = self.flow[key]   # [18, 768]\n",
    "        audio = self.audio[key] # [18, 128]\n",
    "\n",
    "        video_feat = np.concatenate([rgb, flow], axis=-1)  # [18, 1536]\n",
    "        full_feat = np.concatenate([video_feat, audio], axis=-1)  # [18, 1664]\n",
    "        full_feat = torch.tensor(full_feat, dtype=torch.float32)\n",
    "\n",
    "        label = self.label_map[key[0]]\n",
    "        return full_feat.mean(dim=0), torch.tensor(label)  # [1664], int64\n",
    "    \n",
    "class ActionClassificationDatasetVisualOnly(Dataset):\n",
    "    def __init__(self, csv_path, rgb_dict, flow_dict, label_map):\n",
    "        import pandas as pd\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.rgb = rgb_dict\n",
    "        self.flow = flow_dict\n",
    "        self.label_map = label_map\n",
    "\n",
    "        self.keys = []\n",
    "        self.labels = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            vid, cname = row[\"video_id\"], row[\"class_name\"]\n",
    "            key = (cname, vid)\n",
    "            if key in self.rgb and key in self.flow and cname in self.label_map:\n",
    "                self.keys.append(key)\n",
    "                self.labels.append(self.label_map[cname])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        label = self.labels[idx]\n",
    "        rgb = self.rgb[key]\n",
    "        flow = self.flow[key]\n",
    "        video_feat = np.concatenate([rgb, flow], axis=1)  # shape: [18, 768+768]\n",
    "        video_feat = np.mean(video_feat, axis=0)          # [1536]\n",
    "        return torch.tensor(video_feat, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1664, hidden_dim=512, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device, return_preds=False):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds.append(pred.argmax(dim=1).cpu())\n",
    "            targets.append(y.cpu())\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    acc = accuracy_score(targets.numpy(), preds.numpy())\n",
    "    if return_preds:\n",
    "        return total_loss / len(loader.dataset), acc, targets.numpy(), preds.numpy()\n",
    "    return total_loss / len(loader.dataset), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''\n",
    "Ïã§Ìñâ Ïãú Î°úÏª¨ ÌôòÍ≤ΩÍ≥º Í≤ΩÎ°úÎ•º ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\n",
    "'''\n",
    "csv_path = r\"D:\\Audio-Feature\\18-audio-train.csv\"\n",
    "label_map_path = r\"C:\\Users\\swu\\Desktop\\20_class.txt\"\n",
    "\n",
    "label_map = {}\n",
    "\n",
    "with open(label_map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        name, idx = line.strip().split(\",\")\n",
    "        label_map[name] = int(idx)\n",
    "\n",
    "rgb_train = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\rgb_filtered_train.npy\", allow_pickle=True).item()\n",
    "flow_train = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\flow_filtered_train.npy\", allow_pickle=True).item()\n",
    "gt_audio_train = np.load(r\"D:\\Audio-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\audio_filtered_train.npy\", allow_pickle=True).item()\n",
    "gen_audio_train = np.load(r\"D:\\Gen-Audio\\gen_audio_v1_train.npy\", allow_pickle=True).item()\n",
    "\n",
    "rgb_val = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\rgb_filtered_val.npy\", allow_pickle=True).item()\n",
    "flow_val = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\flow_filtered_val.npy\", allow_pickle=True).item()\n",
    "gt_audio_val = np.load(r\"D:\\Audio-Feature\\training\\3.18frames-audioÏú†Ìö®-split-feature\\audio_filtered_val.npy\", allow_pickle=True).item()\n",
    "gen_audio_val = np.load(r\"D:\\Gen-Audio\\gen_audio_v1_val.npy\", allow_pickle=True).item()\n",
    "\n",
    "vis_train_dataset = ActionClassificationDatasetVisualOnly(csv_path, rgb_train, flow_train, label_map)\n",
    "vis_val_dataset   = ActionClassificationDatasetVisualOnly(csv_path, rgb_val, flow_val, label_map)\n",
    "\n",
    "vis_train_loader = DataLoader(vis_train_dataset, batch_size=32, shuffle=True)\n",
    "vis_val_loader   = DataLoader(vis_val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "gt_train_dataset = ActionClassificationDataset(csv_path, rgb_train, flow_train, gt_audio_train, label_map)\n",
    "gt_val_dataset   = ActionClassificationDataset(csv_path, rgb_val, flow_val, gt_audio_val, label_map)\n",
    "gt_train_loader  = DataLoader(gt_train_dataset, batch_size=32, shuffle=True)\n",
    "gt_val_loader    = DataLoader(gt_val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "gen_train_dataset = ActionClassificationDataset(csv_path, rgb_train, flow_train, gen_audio_train, label_map)\n",
    "gen_val_dataset   = ActionClassificationDataset(csv_path, rgb_val, flow_val, gen_audio_val, label_map)\n",
    "gen_train_loader  = DataLoader(gen_train_dataset, batch_size=32, shuffle=True)\n",
    "gen_val_loader    = DataLoader(gen_val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ActionClassifier(input_dim=1664).to(device)\n",
    "\n",
    "model_gt   = ActionClassifier(input_dim=1664).to(device)\n",
    "model_gen  = ActionClassifier(input_dim=1664).to(device)\n",
    "model_vis  = ActionClassifier(input_dim=1536).to(device) \n",
    "\n",
    "optimizer_gt  = torch.optim.Adam(model_gt.parameters(), lr=1e-4)\n",
    "optimizer_gen = torch.optim.Adam(model_gen.parameters(), lr=1e-4)\n",
    "optimizer_vis = torch.optim.Adam(model_vis.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss_gt  = train(model_gt,  gt_train_loader,  optimizer_gt,  criterion, device)\n",
    "    train_loss_gen = train(model_gen, gen_train_loader, optimizer_gen, criterion, device)\n",
    "    train_loss_vis = train(model_vis, vis_train_loader, optimizer_vis, criterion, device)\n",
    "\n",
    "    val_loss_gt,  val_acc_gt  = evaluate(model_gt,  gt_val_loader,  criterion, device)\n",
    "    val_loss_gen, val_acc_gen = evaluate(model_gen, gen_val_loader, criterion, device)\n",
    "    val_loss_vis, val_acc_vis = evaluate(model_vis, vis_val_loader, criterion, device)\n",
    "\n",
    "    print('Audio Feature V1')\n",
    "    print(f\"[Epoch {epoch+1}]\")\n",
    "    print(f\"GT     | Train Loss: {train_loss_gt:.4f} | Val Loss: {val_loss_gt:.4f} | Val Acc: {val_acc_gt:.4f}\")\n",
    "    print(f\"Gen    | Train Loss: {train_loss_gen:.4f} | Val Loss: {val_loss_gen:.4f} | Val Acc: {val_acc_gen:.4f}\")\n",
    "    print(f\"Visual | Train Loss: {train_loss_vis:.4f} | Val Loss: {val_loss_vis:.4f} | Val Acc: {val_acc_vis:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"adult+female+singing\", \"adult+female+speaking\", \"adult+male+singing\", \"adult+male+speaking\",\n",
    "    \"applauding\", \"ascending\", \"asking\", \"assembling\", \"autographing\", \"baking\",\n",
    "    \"balancing\", \"barbecuing\", \"barking\", \"bending\", \"bicycling\", \"biting\",\n",
    "    \"blowing\", \"boarding\", \"boating\", \"boiling\"\n",
    "]\n",
    "\n",
    "_, _, y_true_gt, y_pred_gt = evaluate(model_gt, gt_val_loader, criterion, device, return_preds=True)\n",
    "plot_confusion_matrix(y_true_gt, y_pred_gt, class_names, title=\"Confusion Matrix (GT Audio)\")\n",
    "\n",
    "_, _, y_true_gen, y_pred_gen = evaluate(model_gen, gen_val_loader, criterion, device, return_preds=True)\n",
    "plot_confusion_matrix(y_true_gen, y_pred_gen, class_names, title=\"Confusion Matrix (Generated Audio)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TeamPossible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
