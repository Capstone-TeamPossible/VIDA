{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **비디오 데이터 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 기존 모듈 정의 (VideoPatchEmbed, VideoEmbed, VideoViTFeatureExtractor, VideoFeatureExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPatchEmbed(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = (patch_size, patch_size)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = rearrange(x, 'b c t h w -> (b t) c h w')\n",
    "        x = self.proj(x) \n",
    "        _, embed_dim, H_patch, W_patch = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2) \n",
    "        return x, T, W_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEmbed(nn.Module):\n",
    "    def __init__(self, patch_embed, embed_dim=768, num_frames=18):\n",
    "        super().__init__()\n",
    "        self.patch_embed = patch_embed\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_frames = num_frames\n",
    "        self.time_embed = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        nn.init.trunc_normal_(self.time_embed, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x, T, W_patch = self.patch_embed(x)\n",
    "        N = x.shape[1]\n",
    "        x = x.view(B, T, N, self.embed_dim)\n",
    "        if T != self.time_embed.shape[1]:\n",
    "            time_embed = self.time_embed.transpose(1, 2)\n",
    "            new_time_embed = F.interpolate(time_embed, size=T, mode='linear', align_corners=False)\n",
    "            new_time_embed = new_time_embed.transpose(1, 2)\n",
    "        else:\n",
    "            new_time_embed = self.time_embed\n",
    "        x = x + new_time_embed.unsqueeze(2)\n",
    "        x = self.drop(x)\n",
    "        x = x.reshape(B, T * N, self.embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "class VideoViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, embed_dim=768, depth=12, num_heads=12, mlp_dim=3072, num_tokens=196, dropout=0.1):\n",
    "        super().__init__()\n",
    "        config = ViTConfig(\n",
    "            hidden_size=embed_dim,\n",
    "            num_hidden_layers=depth,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=mlp_dim,\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout,\n",
    "            image_size=224, \n",
    "            patch_size=1,    \n",
    "            num_channels=3,   \n",
    "        )\n",
    "        self.vit = ViTModel(config)\n",
    "        self.vit.embeddings.patch_embeddings = nn.Identity()\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        num_total_tokens = num_tokens + 1  \n",
    "        self.vit.embeddings.position_embeddings = nn.Parameter(torch.zeros(1, num_total_tokens, embed_dim))\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.vit.embeddings.position_embeddings, std=0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        B, total_tokens, D = tokens.shape\n",
    "        T = 18  \n",
    "        N = total_tokens // T\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)           \n",
    "        x = torch.cat((cls_tokens, tokens), dim=1)             \n",
    "        x = x + self.vit.embeddings.position_embeddings[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        encoder_outputs = self.vit.encoder(x, return_dict=True)\n",
    "        x = self.vit.layernorm(encoder_outputs.last_hidden_state[:, 1:]) \n",
    "\n",
    "        x = x.view(B, T, N, D)      \n",
    "        frame_features = x.mean(dim=2) \n",
    "        return frame_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_chans=3, patch_size=16, embed_dim=768, num_frames=18, vit_depth=12, vit_heads=12, vit_mlp_dim=3072):\n",
    "        super().__init__()\n",
    "        self.video_embed = VideoEmbed(\n",
    "            VideoPatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim),\n",
    "            embed_dim=embed_dim,\n",
    "            num_frames=num_frames\n",
    "        )\n",
    "        num_patches_per_frame = (224 // patch_size) ** 2\n",
    "        total_tokens = num_frames * num_patches_per_frame\n",
    "        self.vit_extractor = VideoViTFeatureExtractor(\n",
    "            embed_dim=embed_dim,\n",
    "            depth=vit_depth,\n",
    "            num_heads=vit_heads,\n",
    "            mlp_dim=vit_mlp_dim,\n",
    "            num_tokens=total_tokens,\n",
    "            dropout=0.1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tokens = self.video_embed(x)\n",
    "        features = self.vit_extractor(tokens)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 이미지 로딩 및 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(folder, modality='rgb', num_frames=18, target_size=(224,224)):\n",
    "    \"\"\"\n",
    "    folder: 비디오 프레임들이 저장된 폴더 경로  \n",
    "    modality: 'rgb'이면 .jpg, 'flow'이면 .png 파일 읽음  \n",
    "    \"\"\"\n",
    "    if modality == 'rgb':\n",
    "        ext = '*.jpg'\n",
    "    elif modality == 'flow':\n",
    "        ext = '*.png'\n",
    "    else:\n",
    "        raise ValueError(\"알 수 없는 modality입니다.\")\n",
    "    files = sorted(glob.glob(os.path.join(folder, ext)))\n",
    "    files = files[:num_frames]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        if modality == 'rgb':\n",
    "            img = Image.open(f).convert('RGB')\n",
    "            img = transform(img)\n",
    "        else:\n",
    "            img = Image.open(f).convert('L')\n",
    "            img = img.convert('RGB')  \n",
    "            img = transform(img)\n",
    "            img = img[:2, :, :] \n",
    "        frames.append(img)\n",
    "    video = torch.stack(frames, dim=1) \n",
    "    video = video.unsqueeze(0) \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 폴더 순회 및 여러 비디오에 대해 피처 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(modality, root_dir, extractor, device, num_frames=18, target_size=(224,224), target_classes=None):\n",
    "    results = {}\n",
    "    for class_name in os.listdir(root_dir):\n",
    "        if target_classes is not None and class_name not in target_classes:\n",
    "            continue\n",
    "\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        frames_folder = os.path.join(class_dir, \"18frames\")\n",
    "        if not os.path.isdir(frames_folder):\n",
    "            print(f\"'18frames' 폴더가 {class_dir}에 없습니다.\")\n",
    "            continue\n",
    "        for video_name in os.listdir(frames_folder):\n",
    "            video_dir = os.path.join(frames_folder, video_name)\n",
    "            if not os.path.isdir(video_dir):\n",
    "                continue\n",
    "            video_id = video_name \n",
    "            video = load_video_frames(video_dir, modality=modality, num_frames=num_frames, target_size=target_size).to(device)\n",
    "            with torch.no_grad():\n",
    "                features = extractor(video)\n",
    "            key = (class_name, video_id)\n",
    "            results[key] = features.squeeze(0).cpu().numpy() \n",
    "            print(f\"처리 완료: {modality}/{class_name}/{video_id}, 피처 shape: {features.shape}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 메인 실행: 모델 생성 후 데이터 전체에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    rgb_extractor = VideoFeatureExtractor(in_chans=3).to(device)\n",
    "    flow_extractor = VideoFeatureExtractor(in_chans=2).to(device)\n",
    "    \n",
    "    '''\n",
    "    실행 시 로컬 환경과 경로를 맞춰주세요.\n",
    "    '''\n",
    "    root_rgb = r\"D:\\RGB\\training\"\n",
    "    root_flow = r\"D:\\OpticalFlow\\training\"\n",
    "\n",
    "    target_classes = [\n",
    "        \"adult+female+singing\", \"adult+female+speaking\", \"adult+male+singing\",\n",
    "        \"adult+male+speaking\", \"applauding\", \"ascending\", \"asking\", \"assembling\",\n",
    "        \"autographing\", \"baking\", \"balancing\", \"barbecuing\", \"barking\", \"bending\",\n",
    "        \"bicycling\", \"biting\", \"blowing\", \"boarding\", \"boating\", \"boiling\"\n",
    "    ]\n",
    "\n",
    "    rgb_results = process_videos('rgb', root_rgb, rgb_extractor, device, target_classes=target_classes)\n",
    "    flow_results = process_videos('flow', root_flow, flow_extractor, device, target_classes=target_classes)\n",
    "\n",
    "    np.save(\"rgb_training.npy\", rgb_results)\n",
    "    np.save(\"flow_training.npy\", flow_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1. 추출 데이터 검증 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_train = np.load(\"rgb_training.npy\", allow_pickle=True).item()\n",
    "\n",
    "for (class_name, video_id), feature in rgb_train.items():\n",
    "    print(f\"이 영상의 라벨은 → {class_name}\")\n",
    "    print(f\"이 영상의 ID는 → {video_id}\")\n",
    "    print(f\"피처 shape은 → {feature.shape}\")  # (18, 768)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. audio유효 데이터 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def filter_by_valid_audio(npy_path, csv_path, save_path):\n",
    "    valid_keys = set()\n",
    "    with open(csv_path, newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            video_id = row['video_id']\n",
    "            class_name = row['class_name']\n",
    "            valid_keys.add((class_name, video_id))\n",
    "\n",
    "    print(f\"유효한 오디오 키 수: {len(valid_keys)}\")\n",
    "\n",
    "    full_data = np.load(npy_path, allow_pickle=True).item()\n",
    "    print(f\"전체 데이터 수: {len(full_data)}\")\n",
    "\n",
    "    filtered = {k: v for k, v in full_data.items() if k in valid_keys}\n",
    "    print(f\"필터링 후 남은 수: {len(filtered)}\")\n",
    "\n",
    "    np.save(save_path, filtered)\n",
    "    print(f\"저장 완료: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 시 로컬 환경과 경로를 맞춰주세요.\n",
    "'''\n",
    "csv_path = r'D:\\Audio-Feature\\18-audio-train.csv'\n",
    "filter_by_valid_audio(r\"D:\\Video-Feature\\training\\18frames-feature\\rgb_training.npy\", csv_path, r\"D:\\Video-Feature\\training\\18frames-audio유효-feature\\rgb_training_filtered.npy\")\n",
    "filter_by_valid_audio(r\"D:\\Video-Feature\\training\\18frames-feature\\flow_training.npy\", csv_path, r\"D:\\Video-Feature\\training\\18frames-audio유효-feature\\flow_training_filtered.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_by_class(data_dict, name=\"\"):\n",
    "    class_counts = defaultdict(int)\n",
    "    for (class_name, video_id) in data_dict:\n",
    "        class_counts[class_name] += 1\n",
    "\n",
    "    print(f\"{name} 총 샘플 수: {len(data_dict)}\")\n",
    "    for class_name in sorted(class_counts):\n",
    "        print(f\"  - {class_name}: {class_counts[class_name]}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 시 로컬 환경과 경로를 맞춰주세요.\n",
    "'''\n",
    "rgb_filtered = r\"D:\\Video-Feature\\training\\18frames-audio유효-feature\\rgb_training_filtered.npy\"\n",
    "optical_filtered = r\"D:\\Video-Feature\\training\\18frames-audio유효-feature\\flow_training_filtered.npy\"\n",
    "\n",
    "rgb_data = np.load(rgb_filtered, allow_pickle=True).item()\n",
    "flow_data = np.load(optical_filtered, allow_pickle=True).item()\n",
    "\n",
    "count_by_class(rgb_data, \"RGB\")\n",
    "count_by_class(flow_data, \"Optical Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. train/validation/test 셋 분리 (클래스 별 균형)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def split_keys_by_class(data_dict, ratios=(0.7, 0.15, 0.15), seed=42):\n",
    "    train_keys, val_keys, test_keys = [], [], []\n",
    "    class_groups = defaultdict(list)\n",
    "\n",
    "    for key in data_dict:\n",
    "        class_name, video_id = key\n",
    "        class_groups[class_name].append(key)\n",
    "\n",
    "    for class_name, keys in class_groups.items():\n",
    "        random.Random(seed).shuffle(keys)\n",
    "        n = len(keys)\n",
    "        n_train = int(n * ratios[0])\n",
    "        n_val = int(n * ratios[1])\n",
    "        n_test = n - n_train - n_val\n",
    "\n",
    "        train_keys += keys[:n_train]\n",
    "        val_keys += keys[n_train:n_train + n_val]\n",
    "        test_keys += keys[n_train + n_val:]\n",
    "\n",
    "    return train_keys, val_keys, test_keys\n",
    "\n",
    "def check_split_ratio(train_dict, val_dict, test_dict):\n",
    "    counter = defaultdict(lambda: [0, 0, 0])\n",
    "\n",
    "    for cls, _ in train_dict: counter[cls][0] += 1\n",
    "    for cls, _ in val_dict: counter[cls][1] += 1\n",
    "    for cls, _ in test_dict: counter[cls][2] += 1\n",
    "\n",
    "    print(f\"{'Class':35s} | Train | Val | Test | Total | Train% | Val% | Test%\")\n",
    "    print(\"-\"*85)\n",
    "    for cls in sorted(counter):\n",
    "        t, v, ts = counter[cls]\n",
    "        total = t + v + ts\n",
    "        print(f\"{cls:35s} | {t:5d} | {v:3d} | {ts:4d} | {total:5d} | {t/total:6.2%} | {v/total:5.2%} | {ts/total:6.2%}\")\n",
    "\n",
    "def split_and_save_filtered(rgb_path, flow_path):\n",
    "    print(\"필터된 데이터 로딩 중...\")\n",
    "    rgb_all = np.load(rgb_path, allow_pickle=True).item()\n",
    "    flow_all = np.load(flow_path, allow_pickle=True).item()\n",
    "\n",
    "    assert set(rgb_all.keys()) == set(flow_all.keys()), \"RGB / Flow 키 불일치!\"\n",
    "\n",
    "    train_keys, val_keys, test_keys = split_keys_by_class(rgb_all)\n",
    "\n",
    "    rgb_train = {k: rgb_all[k] for k in train_keys}\n",
    "    rgb_val   = {k: rgb_all[k] for k in val_keys}\n",
    "    rgb_test  = {k: rgb_all[k] for k in test_keys}\n",
    "\n",
    "    flow_train = {k: flow_all[k] for k in train_keys}\n",
    "    flow_val   = {k: flow_all[k] for k in val_keys}\n",
    "    flow_test  = {k: flow_all[k] for k in test_keys}\n",
    "\n",
    "    save_dir = r\"D:\\Video-Feature\\training\\18frames-audio유효-split-feature\"\n",
    "    \n",
    "    np.save(os.path.join(save_dir, \"rgb_filtered_train.npy\"), rgb_train)\n",
    "    np.save(os.path.join(save_dir, \"rgb_filtered_val.npy\"), rgb_val)\n",
    "    np.save(os.path.join(save_dir, \"rgb_filtered_test.npy\"), rgb_test)\n",
    "\n",
    "    np.save(os.path.join(save_dir, \"flow_filtered_train.npy\"), flow_train)\n",
    "    np.save(os.path.join(save_dir, \"flow_filtered_val.npy\"), flow_val)\n",
    "    np.save(os.path.join(save_dir, \"flow_filtered_test.npy\"), flow_test)\n",
    "\n",
    "    print(\"저장 완료. 클래스 분포:\")\n",
    "    check_split_ratio(rgb_train, rgb_val, rgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_save_filtered(rgb_filtered, optical_filtered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TeamPossible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
