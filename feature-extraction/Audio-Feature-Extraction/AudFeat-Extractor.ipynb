{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **오디오 데이터 추출**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **사전 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import ASTFeatureExtractor, ASTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전 학습 모델 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model = ASTModel.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 사용 확인 및 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available()) \n",
    "print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. .wav 파일 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def load_audio(path):\n",
    "    waveform, sr = sf.read(path, dtype='float32')\n",
    "    waveform = torch.from_numpy(waveform).squeeze() \n",
    "    return waveform, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1-1. waveform segmenting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def split_waveform(waveform, num_segments=18):\n",
    "    T = waveform.shape[0]\n",
    "    target_length = int(np.ceil(T / num_segments)) * num_segments \n",
    "    pad_len = target_length - T\n",
    "    if pad_len > 0:\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    segment_length = waveform.shape[0] // num_segments\n",
    "    return waveform.split(segment_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Spectrogram 변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram_segments(segments, sr=16000):\n",
    "    specs = []\n",
    "    for segment in segments:\n",
    "        inputs = feature_extractor(\n",
    "            segment,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        specs.append(inputs)\n",
    "    return specs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. ASTModel에 입력 & 4. [CLS] 임베딩 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forward_ast_sequence(spec_input_list):\n",
    "    input_values = torch.cat([inp[\"input_values\"] for inp in spec_input_list], dim=0)  \n",
    "    outputs = model(input_values)  \n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :] \n",
    "    return cls_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. 128-dim Linear Projection & 6. .npy 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = torch.nn.Linear(768, 128).cuda()\n",
    "projector.eval()\n",
    "\n",
    "def project_sequence_feature(patch_sequence):\n",
    "    feature_seq = projector(patch_sequence) \n",
    "    feature_seq = feature_seq.detach().cpu().numpy() \n",
    "    return feature_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **피처 추출 및 딕셔너리 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def process_and_save_features(audio_dir, target_classes):\n",
    "    results = {}\n",
    "\n",
    "    for class_name in os.listdir(audio_dir):\n",
    "        if class_name not in target_classes:\n",
    "            continue\n",
    "\n",
    "        input_class_dir = os.path.join(audio_dir, class_name, '18frames')\n",
    "        if not os.path.exists(input_class_dir):\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(input_class_dir):\n",
    "            if not filename.endswith(\".wav\"):\n",
    "                continue\n",
    "\n",
    "            audio_path = os.path.join(input_class_dir, filename)\n",
    "            video_id = os.path.splitext(filename)[0]\n",
    "            try:\n",
    "                waveform, sr = load_audio(audio_path)\n",
    "                segments = split_waveform(waveform, num_segments=18)\n",
    "                inputs = extract_spectrogram_segments(segments, sr=16000)\n",
    "                segments_embedding = forward_ast_sequence(inputs)\n",
    "                features = project_sequence_feature(segments_embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {audio_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            key = (class_name, video_id)\n",
    "            results[key] = features\n",
    "            print(f\"처리 완료: {class_name}/{video_id}, 피처 shape: {features.shape}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    '''\n",
    "    실행 시 로컬 환경과 경로를 맞춰주세요.\n",
    "    target_class는 실험 시 데이터 범위에 따라 임의로 설정해주세요.\n",
    "    '''\n",
    "    audio_dir = root_rgb = r\"D:\\Audio\\training\"\n",
    "    target_classes = [\n",
    "            \"adult+female+singing\", \"adult+female+speaking\", \"adult+male+singing\",\n",
    "            \"adult+male+speaking\", \"applauding\", \"ascending\", \"asking\", \"assembling\",\n",
    "            \"autographing\", \"baking\", \"balancing\", \"barbecuing\", \"barking\", \"bending\",\n",
    "            \"bicycling\", \"biting\", \"blowing\", \"boarding\", \"boating\", \"boiling\"\n",
    "        ]\n",
    "    \n",
    "    results = process_and_save_features(audio_dir, target_classes)\n",
    "    np.save(\"audio_filtered.npy\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train/test/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "실행 시 로컬 환경과 경로를 맞춰주세요.\n",
    "'''\n",
    "audio_dict = np.load(\"audio_filtered.npy\", allow_pickle=True).item()\n",
    "train_rgb = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audio유효-split-feature\\rgb_filtered_train.npy\", allow_pickle=True).item()\n",
    "val_rgb = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audio유효-split-feature\\rgb_filtered_val.npy\", allow_pickle=True).item()\n",
    "test_rgb = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audio유효-split-feature\\rgb_filtered_test.npy\", allow_pickle=True).item()\n",
    "\n",
    "train_keys = set(train_rgb.keys())\n",
    "val_keys = set(val_rgb.keys())\n",
    "test_keys = set(test_rgb.keys())\n",
    "\n",
    "train_audio = {k: v for k, v in audio_dict.items() if k in train_keys}\n",
    "val_audio = {k: v for k, v in audio_dict.items() if k in val_keys}\n",
    "test_audio = {k: v for k, v in audio_dict.items() if k in test_keys}\n",
    "\n",
    "np.save(\"audio_filtered_train.npy\", train_audio)\n",
    "np.save(\"audio_filtered_val.npy\", val_audio)\n",
    "np.save(\"audio_filtered_test.npy\", test_audio)\n",
    "\n",
    "print(f\"분할 완료: train={len(train_audio)}, val={len(val_audio)}, test={len(test_audio)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### waveform 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import soundfile as sf\n",
    "\n",
    "def load_audio(path):\n",
    "    waveform, sr = sf.read(path, dtype='float32', always_2d=True)  \n",
    "    waveform = torch.from_numpy(waveform[:, 0])  \n",
    "    return waveform, sr\n",
    "\n",
    "def pad_waveform_ceil_18(waveform, num_segments=18):\n",
    "    T = waveform.shape[0]\n",
    "    target_length = int(np.ceil(T / num_segments)) * num_segments  \n",
    "    pad_len = target_length - T\n",
    "    if pad_len > 0:\n",
    "        waveform = F.pad(waveform, (0, pad_len)) \n",
    "    return waveform \n",
    "\n",
    "def save_waveform(audio_dir, target_classes):\n",
    "    results = {}\n",
    "\n",
    "    for class_name in os.listdir(audio_dir):\n",
    "        if class_name not in target_classes:\n",
    "            continue\n",
    "\n",
    "        input_class_dir = os.path.join(audio_dir, class_name, '18frames')\n",
    "        if not os.path.exists(input_class_dir):\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(input_class_dir):\n",
    "            if not filename.endswith(\".wav\"):\n",
    "                continue\n",
    "\n",
    "            audio_path = os.path.join(input_class_dir, filename)\n",
    "            video_id = os.path.splitext(filename)[0]\n",
    "            try:\n",
    "                waveform, sr = load_audio(audio_path)\n",
    "                waveform = pad_waveform_ceil_18(waveform)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {audio_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            key = (class_name, video_id)\n",
    "            results[key] = waveform\n",
    "            print(f\"저장 완료: {class_name}/{video_id}, padded length: {waveform.shape[0]}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    '''\n",
    "    실행 시 로컬 환경과 경로를 맞춰주세요.\n",
    "    '''\n",
    "    audio_dir = root_rgb = r\"D:\\Audio\\training\"\n",
    "    target_classes = [\n",
    "            \"adult+female+singing\", \"adult+female+speaking\", \"adult+male+singing\",\n",
    "            \"adult+male+speaking\", \"applauding\", \"ascending\", \"asking\", \"assembling\",\n",
    "            \"autographing\", \"baking\", \"balancing\", \"barbecuing\", \"barking\", \"bending\",\n",
    "            \"bicycling\", \"biting\", \"blowing\", \"boarding\", \"boating\", \"boiling\"\n",
    "        ]\n",
    "    \n",
    "    results = save_waveform(audio_dir, target_classes)\n",
    "    np.save(\"audio_waveform.npy\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "waveform_dict = np.load(\"audio_waveform.npy\", allow_pickle=True).item()\n",
    "\n",
    "lengths = [v.shape[0] for v in waveform_dict.values()]\n",
    "\n",
    "length_counts = Counter(lengths)\n",
    "\n",
    "for length in sorted(length_counts.keys()):\n",
    "    print(f\"Length: {length}, Count: {length_counts[length]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "실행 시 로컬 환경과 경로를 맞춰주세요.\n",
    "'''\n",
    "audio_dict = np.load(\"audio_waveform.npy\", allow_pickle=True).item()\n",
    "train_rgb = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audio유효-split-feature\\rgb_filtered_train.npy\", allow_pickle=True).item()\n",
    "val_rgb = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audio유효-split-feature\\rgb_filtered_val.npy\", allow_pickle=True).item()\n",
    "test_rgb = np.load(r\"D:\\Video-Feature\\training\\3.18frames-audio유효-split-feature\\rgb_filtered_test.npy\", allow_pickle=True).item()\n",
    "\n",
    "train_keys = set(train_rgb.keys())\n",
    "val_keys = set(val_rgb.keys())\n",
    "test_keys = set(test_rgb.keys())\n",
    "\n",
    "train_audio = {k: v for k, v in audio_dict.items() if k in train_keys}\n",
    "val_audio = {k: v for k, v in audio_dict.items() if k in val_keys}\n",
    "test_audio = {k: v for k, v in audio_dict.items() if k in test_keys}\n",
    "\n",
    "np.save(\"audio_waveform_train.npy\", train_audio)\n",
    "np.save(\"audio_waveform_val.npy\", val_audio)\n",
    "np.save(\"audio_waveform_test.npy\", test_audio)\n",
    "\n",
    "print(f\"분할 완료: train={len(train_audio)}, val={len(val_audio)}, test={len(test_audio)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TeamPossible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
